## [Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples](https://arxiv.org/abs/1602.02697)


### Key Points
- This paper attacks against black-box DNN classifiers without knowledge of the classifier training data or model.
- The black-box attack evades defenses proposed in the literature because the substitute trained by the adversary is unaffected by defenses deployed on
the targeted oracle model to reduce its vulnerability

### Method
Since adversarial examples transfer between architectures, we can 
1. learning a substitute DNN approximating the target using a dataset constructed with synthetic inputs and labels observed from the oracle
2. Craft adversarial example using this substitute


### Exp


### Thought


### Questions

## [Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples](https://arxiv.org/abs/1602.02697)


### Key Points
- This paper attacks against black-box DNN classifiers without knowledge of the classifier training data or model.
- The adversary has no knowledge of the architectural choices made to design the DNN, which include the number, type, and size of layers, 
nor of the training data used to learn the DNNâ€™s parameters.

### Model



### Exp


### Thought


### Questions

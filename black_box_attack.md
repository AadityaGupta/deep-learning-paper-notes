## [Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples](https://arxiv.org/abs/1602.02697)


### Key Points
- This paper attacks against black-box DNN classifiers without knowledge of the classifier training data or model.
- The black-box attack evades defenses proposed in the literature because the substitute trained by the adversary is unaffected by defenses deployed on
the targeted oracle model to reduce its vulnerability

### Model



### Exp


### Thought


### Questions
